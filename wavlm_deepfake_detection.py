# -*- coding: utf-8 -*-
"""WavLM Deepfake Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1at7zw3WcnNs0-1DSXSXCd_LYjd_AzslD
"""

!python --version

!pip3 install torch torchvision torchaudio transformers kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("bhaveshkumars/release-in-the-wild")

print("Path to dataset files:", path)

import os

for root, dirs, files in os.walk(path):
    print(f"{root} | dirs: {dirs} | files: {len(files)}")

import torch
import torchaudio
import os
from pathlib import Path
from torch.utils.data import Dataset, DataLoader

# --- 1. Define Paths and Constants ---
# Base directory of the dataset
BASE_DIR = Path(path) / 'release_in_the_wild'

# Define paths for each split
TRAIN_DIR = BASE_DIR / 'train'
VAL_DIR = BASE_DIR / 'val'
TEST_DIR = BASE_DIR / 'test'

# Define target sample rate and other constants
TARGET_SAMPLE_RATE = 16000
BATCH_SIZE = 32 # You can adjust this based on your GPU memory

# --- 2. Create a Custom Dataset Class ---
class AudioDataset(Dataset):
    """
    Custom PyTorch Dataset for loading audio files from a directory structure
    like:
    - data_path/
      - class_a/
        - audio1.wav
        - audio2.wav
      - class_b/
        - audio3.wav
    """
    def __init__(self, data_path: Path, target_sample_rate: int):
        self.data_path = data_path
        self.target_sample_rate = target_sample_rate

        # Mapping from class name (folder name) to an integer label
        self.class_map = {"fake": 0, "real": 1}

        # Find all .wav files and store their paths and labels
        self.file_list = []
        for class_name, label in self.class_map.items():
            class_dir = self.data_path / class_name
            for file_path in class_dir.glob('*.wav'):
                self.file_list.append((str(file_path), label))

    def __len__(self):
        """Returns the total number of files in the dataset."""
        return len(self.file_list)

    def __getitem__(self, index):
        """
        Loads and returns a single sample (waveform, label) from the dataset.
        """
        audio_path, label = self.file_list[index]

        # Load the audio file
        try:
            waveform, original_sample_rate = torchaudio.load(audio_path)
        except Exception as e:
            print(f"Error loading file {audio_path}: {e}")
            # Return a dummy sample or skip
            return torch.zeros(1, self.target_sample_rate), -1 # Dummy sample

        # Resample the waveform if necessary
        if original_sample_rate != self.target_sample_rate:
            resampler = torchaudio.transforms.Resample(
                orig_freq=original_sample_rate,
                new_freq=self.target_sample_rate
            )
            waveform = resampler(waveform)

        # Ensure audio is mono by averaging channels if it's stereo
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        return waveform, label

# --- 3. Define a Collate Function for Padding ---
def collate_fn_audio(batch):
    """
    Pads audio tensors in a batch to the same length.
    `batch` is a list of tuples (waveform, label).
    """
    # Separate waveforms and labels
    tensors, targets = zip(*batch)

    # Pad the audio tensors
    # `pad_sequence` expects a list of tensors and pads them to the max length.
    # It expects tensors of shape (Length, ...), but ours are (Channels, Length).
    # We transpose, pad, and then transpose back.
    tensors_tr = [t.T for t in tensors]
    tensors_padded_tr = torch.nn.utils.rnn.pad_sequence(
        tensors_tr, batch_first=True, padding_value=0.0
    )
    # Transpose back to (Batch, Channels, Length)
    tensors_padded = tensors_padded_tr.permute(0, 2, 1)

    # Stack labels into a tensor
    targets = torch.tensor(targets, dtype=torch.long)

    return tensors_padded, targets

# --- 4. Create Dataset and DataLoader Instances ---
print("Setting up datasets...")
train_dataset = AudioDataset(data_path=TRAIN_DIR, target_sample_rate=TARGET_SAMPLE_RATE)
val_dataset = AudioDataset(data_path=VAL_DIR, target_sample_rate=TARGET_SAMPLE_RATE)
test_dataset = AudioDataset(data_path=TEST_DIR, target_sample_rate=TARGET_SAMPLE_RATE)

print(f"Found {len(train_dataset)} training samples.")
print(f"Found {len(val_dataset)} validation samples.")
print(f"Found {len(test_dataset)} test samples.")

print("\nCreating DataLoaders...")
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True, # Shuffle training data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2 # Use multiple cores for loading
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False, # No need to shuffle validation data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False, # No need to shuffle test data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2
)

print("DataLoaders are ready!")

# --- 5. Example Usage: Inspect a batch from the training loader ---
print("\n--- Verifying the output of the DataLoader ---")
try:
    # Get one batch of data
    audio_batch, labels_batch = next(iter(train_loader))

    print(f"Shape of one batch of audio waveforms: {audio_batch.shape}")
    print(f"Shape of one batch of labels: {labels_batch.shape}")
    print(f"Data type of waveforms: {audio_batch.dtype}")
    print(f"Data type of labels: {labels_batch.dtype}")
    print(f"\nExample labels in the batch: {labels_batch[:5]}")
    print("\nSetup successful.")

except Exception as e:
    print(f"\nAn error occurred during verification: {e}")
    print("Please check file paths and permissions.")

os.environ["HF_TOKEN"] = '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'

# Load model directly
from transformers import AutoModel

model = AutoModel.from_pretrained("microsoft/wavlm-base")

import torch.nn as nn
import torch

class WavLMForClassification(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.wavlm = model
        self.classifier = nn.Linear(768, 2)  # 768 is the hidden size of wavlm-base

    def forward(self, input_values):
        outputs = self.wavlm(input_values)
        # Take the mean of the hidden states across the sequence dimension
        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)
        logits = self.classifier(pooled_output)
        return logits

# Instantiate the new model with the pre-trained WavLM model
model_classifier = WavLMForClassification(model)

import torch.optim as optim
import torch.nn.functional as F

def train_one_epoch(model, train_loader, optimizer, criterion, device):
    model.train()  # Set the model to training mode
    running_loss = 0.0

    for inputs, labels in train_loader:
        # Move data to the specified device
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs.squeeze(1)) # Assuming inputs are [batch, 1, length]

        # Calculate loss
        loss = criterion(outputs, labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        # Accumulate loss
        running_loss += loss.item() * inputs.size(0) # Multiply by batch size

    epoch_loss = running_loss / len(train_loader.dataset)
    return epoch_loss

# Example setup for optimizer, criterion, and device (will be used later in the finetuning script)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_classifier.to(device) # Move model to device

optimizer = optim.AdamW(model_classifier.parameters(), lr=1e-5)
criterion = nn.CrossEntropyLoss()

def evaluate_model(model, val_loader, criterion, device):
    model.eval()  # Set the model to evaluation mode
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():  # Disable gradient calculation
        for inputs, labels in val_loader:
            # Move data to the specified device
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(inputs.squeeze(1)) # Assuming inputs are [batch, 1, length]

            # Calculate loss
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)

            # Calculate correct predictions
            _, predicted = torch.max(outputs.data, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

    epoch_loss = running_loss / total_samples
    accuracy = correct_predictions / total_samples

    return epoch_loss, accuracy

print("evaluate_model function defined.")

import torch.optim as optim
import torch.nn as nn

# Define the optimizer
# We use AdamW, a popular choice for training transformer models
optimizer = optim.AdamW(model_classifier.parameters(), lr=1e-6)

# Define the loss function for binary classification
# CrossEntropyLoss is suitable for multi-class and binary classification tasks
criterion = nn.CrossEntropyLoss()

print("Optimizer and criterion defined.")

import os

# 1. Define the number of training epochs.
num_epochs = 5  # You can adjust this number

# 2. Initialize variables to track the best validation accuracy and the path to save the best model.
best_val_accuracy = -1.0
best_model_path = 'best_wavlm_classifier.pth'

# Ensure the directory for saving the model exists (handle empty dirname case)
save_dir = os.path.dirname(best_model_path)
if save_dir and not os.path.exists(save_dir):
    os.makedirs(save_dir)


print(f"Starting training for {num_epochs} epochs...")

# 3. Loop through the specified number of epochs.
for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    # 4. Inside the loop, call the train_one_epoch function.
    train_loss = train_one_epoch(model_classifier, train_loader, optimizer, criterion, device)
    print(f"  Training Loss: {train_loss:.4f}")

    # 5. After training for an epoch, call the evaluate_model function.
    val_loss, val_accuracy = evaluate_model(model_classifier, val_loader, criterion, device)

    # 6. Print the training loss, validation loss, and validation accuracy.
    print(f"  Validation Loss: {val_loss:.4f}")
    print(f"  Validation Accuracy: {val_accuracy:.4f}")

    # 7. Check if the current validation accuracy is better than the best validation accuracy recorded so far.
    # 8. If the current validation accuracy is better, update the best validation accuracy and save the current model's state dictionary.
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model_classifier.state_dict(), best_model_path)
        print(f"  New best validation accuracy: {best_val_accuracy:.4f}. Model saved to {best_model_path}")

print("\nTraining finished.")
print(f"Best validation accuracy achieved: {best_val_accuracy:.4f}")
print(f"Best model saved to: {best_model_path}")

!nvidia-smi