# -*- coding: utf-8 -*-
"""WavLM Deepfake Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1at7zw3WcnNs0-1DSXSXCd_LYjd_AzslD
"""

!pip3 install torch torchvision torchaudio transformers kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("bhaveshkumars/release-in-the-wild")

print("Path to dataset files:", path)

import os

for root, dirs, files in os.walk(path):
    print(f"{root} | dirs: {dirs} | files: {len(files)}")

import torch
import torchaudio
import os
from pathlib import Path
from torch.utils.data import Dataset, DataLoader

# --- 1. Define Paths and Constants ---
# Base directory of the dataset
BASE_DIR = Path(path) / 'release_in_the_wild'

# Define paths for each split
TRAIN_DIR = BASE_DIR / 'train'
VAL_DIR = BASE_DIR / 'val'
TEST_DIR = BASE_DIR / 'test'

# Define target sample rate and other constants
TARGET_SAMPLE_RATE = 16000
BATCH_SIZE = 8 # You can adjust this based on your GPU memory

# --- 2. Create a Custom Dataset Class ---
class AudioDataset(Dataset):
    """
    Custom PyTorch Dataset for loading audio files from a directory structure
    like:
    - data_path/
      - class_a/
        - audio1.wav
        - audio2.wav
      - class_b/
        - audio3.wav
    """
    def __init__(self, data_path: Path, target_sample_rate: int):
        self.data_path = data_path
        self.target_sample_rate = target_sample_rate

        # Mapping from class name (folder name) to an integer label
        self.class_map = {"fake": 0, "real": 1}

        # Find all .wav files and store their paths and labels
        self.file_list = []
        for class_name, label in self.class_map.items():
            class_dir = self.data_path / class_name
            for file_path in class_dir.glob('*.wav'):
                self.file_list.append((str(file_path), label))

    def __len__(self):
        """Returns the total number of files in the dataset."""
        return len(self.file_list)

    def __getitem__(self, index):
        """
        Loads and returns a single sample (waveform, label) from the dataset.
        """
        audio_path, label = self.file_list[index]

        # Load the audio file
        try:
            waveform, original_sample_rate = torchaudio.load(audio_path)
        except Exception as e:
            print(f"Error loading file {audio_path}: {e}")
            # Return a dummy sample or skip
            return torch.zeros(1, self.target_sample_rate), -1 # Dummy sample

        # Resample the waveform if necessary
        if original_sample_rate != self.target_sample_rate:
            resampler = torchaudio.transforms.Resample(
                orig_freq=original_sample_rate,
                new_freq=self.target_sample_rate
            )
            waveform = resampler(waveform)

        # Ensure audio is mono by averaging channels if it's stereo
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        return waveform, label

# --- 3. Define a Collate Function for Padding ---
def collate_fn_audio(batch):
    """
    Pads audio tensors in a batch to the same length.
    `batch` is a list of tuples (waveform, label).
    """
    # Separate waveforms and labels
    tensors, targets = zip(*batch)

    # Pad the audio tensors
    # `pad_sequence` expects a list of tensors and pads them to the max length.
    # It expects tensors of shape (Length, ...), but ours are (Channels, Length).
    # We transpose, pad, and then transpose back.
    tensors_tr = [t.T for t in tensors]
    tensors_padded_tr = torch.nn.utils.rnn.pad_sequence(
        tensors_tr, batch_first=True, padding_value=0.0
    )
    # Transpose back to (Batch, Channels, Length)
    tensors_padded = tensors_padded_tr.permute(0, 2, 1)

    # Stack labels into a tensor
    targets = torch.tensor(targets, dtype=torch.long)

    return tensors_padded, targets

# --- 4. Create Dataset and DataLoader Instances ---
print("Setting up datasets...")
train_dataset = AudioDataset(data_path=TRAIN_DIR, target_sample_rate=TARGET_SAMPLE_RATE)
val_dataset = AudioDataset(data_path=VAL_DIR, target_sample_rate=TARGET_SAMPLE_RATE)
test_dataset = AudioDataset(data_path=TEST_DIR, target_sample_rate=TARGET_SAMPLE_RATE)

print(f"Found {len(train_dataset)} training samples.")
print(f"Found {len(val_dataset)} validation samples.")
print(f"Found {len(test_dataset)} test samples.")

print("\nCreating DataLoaders...")
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True, # Shuffle training data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2 # Use multiple cores for loading
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False, # No need to shuffle validation data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False, # No need to shuffle test data
    collate_fn=collate_fn_audio,
    num_workers=os.cpu_count() // 2
)

print("DataLoaders are ready!")

# --- 5. Example Usage: Inspect a batch from the training loader ---
print("\n--- Verifying the output of the DataLoader ---")
try:
    # Get one batch of data
    audio_batch, labels_batch = next(iter(train_loader))

    print(f"Shape of one batch of audio waveforms: {audio_batch.shape}")
    print(f"Shape of one batch of labels: {labels_batch.shape}")
    print(f"Data type of waveforms: {audio_batch.dtype}")
    print(f"Data type of labels: {labels_batch.dtype}")
    print(f"\nExample labels in the batch: {labels_batch[:5]}")
    print("\nSetup successful.")

except Exception as e:
    print(f"\nAn error occurred during verification: {e}")
    print("Please check file paths and permissions.")

os.environ["HF_TOKEN"] = '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'

# Load model directly
from transformers import AutoModel

model = AutoModel.from_pretrained("microsoft/wavlm-base")

import torch.nn as nn
import torch
from sklearn.decomposition import PCA

class WavLMFeatureExtractor(nn.Module):
    def __init__(self, model, apply_pooling=True):
        super().__init__()

        self.wavlm = model
        self.apply_pooling = apply_pooling
        if apply_pooling:
            self.avg_pool = nn.AdaptiveAvgPool1d(1)

        # Freeze the parameters of the WavLM model
        for param in self.wavlm.parameters():
            param.requires_grad = False

    def forward(self, input_values):
        # WavLM model expects input of shape (batch_size, sequence_length)
        # Our input_values might be (batch_size, 1, sequence_length), so squeeze the channel dimension
        outputs = self.wavlm(input_values.squeeze(1), output_hidden_states=True)

        # Get the last hidden state
        last_hidden_state = outputs.last_hidden_state

        if self.apply_pooling:
            # Apply average pooling across the sequence length dimension
            # The input to avg_pool should be (batch_size, hidden_size, sequence_length)
            # Permute last_hidden_state from (batch_size, sequence_length, hidden_size)
            pooled_features = self.avg_pool(last_hidden_state.permute(0, 2, 1))
            # Squeeze the pooled dimension to get (batch_size, hidden_size)
            return pooled_features.squeeze(-1)
        else:
            # Return the sequence of hidden states if pooling is not applied
            return last_hidden_state

feature_extractor = WavLMFeatureExtractor(model, apply_pooling=True)

# Check if WavLM parameters are frozen
all_frozen = all(param.requires_grad == False for param in feature_extractor.parameters())
print(f"Are WavLM parameters frozen? {all_frozen}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
feature_extractor.to(device)

print(len(train_loader), len(val_loader), len(test_loader))

!nvidia-smi

import numpy as np

# Initialize lists to store features and labels
train_features = []
train_labels = []
val_features = []
val_labels = []
test_features = []
test_labels = []

# Set the feature extractor to evaluation mode
feature_extractor.eval()

# Extract features and labels for the training set
print("Extracting training features...")
with torch.no_grad():
    for i, (inputs, labels) in enumerate(train_loader):
        if i >= len(train_loader) // 2:
            break
        print(f"Batch {i+1}/{len(train_loader)}")
        inputs = inputs.to(device)
        features = feature_extractor(inputs.squeeze(1)) # Assuming inputs are [batch, 1, length]
        train_features.append(features)
        train_labels.append(labels)

# Extract features and labels for the validation set
print("Extracting validation features...")
with torch.no_grad():
    for i, (inputs, labels) in enumerate(val_loader):
        if i >= len(val_loader) // 2:
            break
        print(f"Batch {i+1}/{len(val_loader)}")
        inputs = inputs.to(device)
        features = feature_extractor(inputs.squeeze(1))
        val_features.append(features)
        val_labels.append(labels)

# Extract features and labels for the test set
print("Extracting test features...")
with torch.no_grad():
    for i, (inputs, labels) in enumerate(test_loader):
        if i >= len(test_loader) // 2:
            break
        print(f"Batch {i+1}/{len(test_loader)}")
        inputs = inputs.to(device)
        features = feature_extractor(inputs.squeeze(1))
        test_features.append(features)
        test_labels.append(labels)

# Concatenate the lists of arrays into single NumPy arrays
train_features = np.concatenate([feat.cpu() for feat in train_features], axis=0)
train_labels = np.concatenate(train_labels, axis=0)
val_features = np.concatenate([feat.cpu() for feat in val_features], axis=0)
val_labels = np.concatenate(val_labels, axis=0)
test_features = np.concatenate([feat.cpu() for feat in test_features], axis=0)
test_labels = np.concatenate(test_labels, axis=0)

# Print the shapes of the resulting arrays
print("\nShapes of extracted data:")
print(f"Train features shape: {train_features.shape}")
print(f"Train labels shape: {train_labels.shape}")
print(f"Validation features shape: {val_features.shape}")
print(f"Validation labels shape: {val_labels.shape}")
print(f"Test features shape: {test_features.shape}")
print(f"Test labels shape: {test_labels.shape}")



!pip install xgboost --upgrade

from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, PredefinedSplit
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
from scipy.stats import uniform, randint

param_dist = {
    'n_estimators': randint(50, 300),  # Random integer between 50 (inclusive) and 300 (exclusive)
    # Use uniform for float ranges
    'learning_rate': uniform(loc=0.01, scale=0.1 - 0.01), # Random float between 0.01 and 0.2
    'max_depth': randint(3, 8),      # Random integer between 3 (inclusive) and 10 (exclusive)
    'subsample': uniform(loc=0.6, scale=0.9 - 0.6), # Random float between 0.6 and 1.0
    'colsample_bytree': uniform(loc=0.6, scale=0.9 - 0.6), # Random float between 0.6 and 1.0
    'gamma': uniform(loc=0.1, scale=0.5),        # Random float between 0 and 1
    'reg_alpha': uniform(loc=0.1, scale=0.5),         # Random float between 0 and 1
    'reg_lambda': uniform(loc=0.1, scale=0.5)         # Random float between 0 and 1
}

# --- 2. Initialize the XGBoost Classifier ---
# We will use this base model in the random search
xgb_base = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)

X_full = np.concatenate([train_features, val_features])
y_full = np.concatenate([train_labels, val_labels])

test_fold = [-1] * len(train_features) + [0] * len(val_features)
ps = PredefinedSplit(test_fold)

# 3. Define your randomized search
random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_dist,
    n_iter=7,
    scoring='accuracy',
    verbose=2,
    random_state=42,
    n_jobs=-1,
    cv=ps
)

# 4. Fit with combined data
random_search.fit(X_full, y_full)

# --- 6. Report the Best Parameters and Score ---
print("\nRandomized Search complete.")
print("Best parameters found: ", random_search.best_params_)
print("Best cross-validation accuracy: {:.4f}".format(random_search.best_score_))

# --- 7. Evaluate on the Test Set with Best Model ---
print("\nEvaluating the model with best parameters on the test set...")
best_xgb_model = random_search.best_estimator_

# Predict on the test set
test_predictions = best_xgb_model.predict(test_features)

# Calculate accuracy on the test set
test_accuracy = accuracy_score(test_labels, test_predictions)
print(f"Test Accuracy (with best parameters): {test_accuracy:.4f}")

# Print a detailed classification report for the test set
print("\nClassification Report (Test Set with best parameters):")
print(classification_report(test_labels, test_predictions))

best_xgb_model.save_model("xgb_model.json")



